{"ast":null,"code":"\"use strict\";\n\nvar _interopRequireDefault = require(\"@babel/runtime/helpers/interopRequireDefault\");\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = createCognitiveServicesSpeechServicesPonyfillFactory;\n\nvar _AudioConfig = require(\"microsoft-cognitiveservices-speech-sdk/distrib/lib/src/sdk/Audio/AudioConfig\");\n\nvar _SpeechServices = _interopRequireDefault(require(\"web-speech-cognitive-services/lib/SpeechServices\"));\n\nfunction createCognitiveServicesSpeechServicesPonyfillFactory(_ref) {\n  var audioConfig = _ref.audioConfig,\n      audioContext = _ref.audioContext,\n      audioInputDeviceId = _ref.audioInputDeviceId,\n      credentials = _ref.credentials,\n      enableTelemetry = _ref.enableTelemetry,\n      speechRecognitionEndpointId = _ref.speechRecognitionEndpointId,\n      speechSynthesisDeploymentId = _ref.speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat = _ref.speechSynthesisOutputFormat,\n      textNormalization = _ref.textNormalization;\n\n  if (!window.navigator.mediaDevices && !audioConfig) {\n    console.warn('botframework-webchat: Your browser does not support Web Audio or the page is not loaded via HTTPS or localhost. Cognitive Services Speech Services is disabled. However, you may pass a custom AudioConfig to enable speech in this environment.');\n    return function () {\n      return {};\n    };\n  }\n\n  if (audioConfig && audioInputDeviceId) {\n    console.warn('botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".');\n  } // WORKAROUND: We should prevent AudioContext object from being recreated because they may be blessed and UX-wise expensive to recreate.\n  //             In Cognitive Services SDK, if they detect the \"end\" function is falsy, they will not call \"end\" but \"suspend\" instead.\n  //             And on next recognition, they will re-use the AudioContext object.\n\n\n  if (!audioConfig) {\n    audioConfig = audioInputDeviceId ? _AudioConfig.AudioConfig.fromMicrophoneInput(audioInputDeviceId) : _AudioConfig.AudioConfig.fromDefaultMicrophoneInput();\n  }\n\n  return function () {\n    var _ref2 = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {},\n        referenceGrammarID = _ref2.referenceGrammarID;\n\n    var _createPonyfill = (0, _SpeechServices.default)({\n      audioConfig: audioConfig,\n      audioContext: audioContext,\n      credentials: credentials,\n      enableTelemetry: enableTelemetry,\n      referenceGrammars: referenceGrammarID ? [\"luis/\".concat(referenceGrammarID, \"-PRODUCTION\")] : [],\n      speechRecognitionEndpointId: speechRecognitionEndpointId,\n      speechSynthesisDeploymentId: speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat: speechSynthesisOutputFormat,\n      textNormalization: textNormalization\n    }),\n        SpeechGrammarList = _createPonyfill.SpeechGrammarList,\n        SpeechRecognition = _createPonyfill.SpeechRecognition,\n        speechSynthesis = _createPonyfill.speechSynthesis,\n        SpeechSynthesisUtterance = _createPonyfill.SpeechSynthesisUtterance;\n\n    return {\n      SpeechGrammarList: SpeechGrammarList,\n      SpeechRecognition: SpeechRecognition,\n      speechSynthesis: speechSynthesis,\n      SpeechSynthesisUtterance: SpeechSynthesisUtterance\n    };\n  };\n}","map":{"version":3,"sources":["../src/createCognitiveServicesSpeechServicesPonyfillFactory.js"],"names":["audioConfig","audioContext","audioInputDeviceId","credentials","enableTelemetry","speechRecognitionEndpointId","speechSynthesisDeploymentId","speechSynthesisOutputFormat","textNormalization","window","console","AudioConfig","referenceGrammarID","SpeechGrammarList","SpeechRecognition","speechSynthesis","SpeechSynthesisUtterance","referenceGrammars"],"mappings":";;;;;;;;;AAAA,IAAA,YAAA,GAAA,OAAA,CAAA,8EAAA,CAAA;;AACA,IAAA,eAAA,GAAA,sBAAA,CAAA,OAAA,CAAA,kDAAA,CAAA,CAAA;;AAEe,SAAA,oDAAA,CAAA,IAAA,EAUZ;AAAA,MATDA,WASC,GAAA,IAAA,CATDA,WASC;AAAA,MARDC,YAQC,GAAA,IAAA,CARDA,YAQC;AAAA,MAPDC,kBAOC,GAAA,IAAA,CAPDA,kBAOC;AAAA,MANDC,WAMC,GAAA,IAAA,CANDA,WAMC;AAAA,MALDC,eAKC,GAAA,IAAA,CALDA,eAKC;AAAA,MAJDC,2BAIC,GAAA,IAAA,CAJDA,2BAIC;AAAA,MAHDC,2BAGC,GAAA,IAAA,CAHDA,2BAGC;AAAA,MAFDC,2BAEC,GAAA,IAAA,CAFDA,2BAEC;AAAA,MADDC,iBACC,GAAA,IAAA,CADDA,iBACC;;AACD,MAAI,CAACC,MAAM,CAANA,SAAAA,CAAD,YAAA,IAAkC,CAAtC,WAAA,EAAoD;AAClDC,IAAAA,OAAO,CAAPA,IAAAA,CAAAA,kPAAAA;AAIA,WAAO,YAAA;AAAA,aAAA,EAAA;AAAP,KAAA;AACD;;AAED,MAAIV,WAAW,IAAf,kBAAA,EAAuC;AACrCU,IAAAA,OAAO,CAAPA,IAAAA,CAAAA,6HAAAA;AAVD,GAAA,CAeD;AACA;AACA;;;AACA,MAAI,CAAJ,WAAA,EAAkB;AAChBV,IAAAA,WAAW,GAAGE,kBAAkB,GAC5BS,YAAAA,CAAAA,WAAAA,CAAAA,mBAAAA,CAD4B,kBAC5BA,CAD4B,GAE5BA,YAAAA,CAAAA,WAAAA,CAFJX,0BAEIW,EAFJX;AAGD;;AAED,SAAO,YAAiC;AAAA,QAAA,KAAA,GAAA,SAAA,CAAA,MAAA,GAAA,CAAA,IAAA,SAAA,CAAA,CAAA,CAAA,KAAA,SAAA,GAAA,SAAA,CAAA,CAAA,CAAA,GAAP,EAAO;AAAA,QAA9BY,kBAA8B,GAAA,KAAA,CAA9BA,kBAA8B;;AAAA,QAAA,eAAA,GACsD,CAAA,GAAA,eAAA,CAAA,OAAA,EAAe;AACzGZ,MAAAA,WAAW,EAD8F,WAAA;AAEzGC,MAAAA,YAAY,EAF6F,YAAA;AAGzGE,MAAAA,WAAW,EAH8F,WAAA;AAIzGC,MAAAA,eAAe,EAJ0F,eAAA;AAKzGa,MAAAA,iBAAiB,EAAEL,kBAAkB,GAAG,CAAA,QAAA,MAAA,CAAA,kBAAA,EAAH,aAAG,CAAA,CAAH,GALoE,EAAA;AAMzGP,MAAAA,2BAA2B,EAN8E,2BAAA;AAOzGC,MAAAA,2BAA2B,EAP8E,2BAAA;AAQzGC,MAAAA,2BAA2B,EAR8E,2BAAA;AASzGC,MAAAA,iBAAiB,EAAjBA;AATyG,KAAf,CADtD;AAAA,QAC9BK,iBAD8B,GAAA,eAAA,CAAA,iBAAA;AAAA,QACXC,iBADW,GAAA,eAAA,CAAA,iBAAA;AAAA,QACQC,eADR,GAAA,eAAA,CAAA,eAAA;AAAA,QACyBC,wBADzB,GAAA,eAAA,CAAA,wBAAA;;AAatC,WAAO;AACLH,MAAAA,iBAAiB,EADZ,iBAAA;AAELC,MAAAA,iBAAiB,EAFZ,iBAAA;AAGLC,MAAAA,eAAe,EAHV,eAAA;AAILC,MAAAA,wBAAwB,EAAxBA;AAJK,KAAP;AAbF,GAAA;AAoBD","sourcesContent":["import { AudioConfig } from 'microsoft-cognitiveservices-speech-sdk/distrib/lib/src/sdk/Audio/AudioConfig';\nimport createPonyfill from 'web-speech-cognitive-services/lib/SpeechServices';\n\nexport default function createCognitiveServicesSpeechServicesPonyfillFactory({\n  audioConfig,\n  audioContext,\n  audioInputDeviceId,\n  credentials,\n  enableTelemetry,\n  speechRecognitionEndpointId,\n  speechSynthesisDeploymentId,\n  speechSynthesisOutputFormat,\n  textNormalization\n}) {\n  if (!window.navigator.mediaDevices && !audioConfig) {\n    console.warn(\n      'botframework-webchat: Your browser does not support Web Audio or the page is not loaded via HTTPS or localhost. Cognitive Services Speech Services is disabled. However, you may pass a custom AudioConfig to enable speech in this environment.'\n    );\n\n    return () => ({});\n  }\n\n  if (audioConfig && audioInputDeviceId) {\n    console.warn(\n      'botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".'\n    );\n  }\n\n  // WORKAROUND: We should prevent AudioContext object from being recreated because they may be blessed and UX-wise expensive to recreate.\n  //             In Cognitive Services SDK, if they detect the \"end\" function is falsy, they will not call \"end\" but \"suspend\" instead.\n  //             And on next recognition, they will re-use the AudioContext object.\n  if (!audioConfig) {\n    audioConfig = audioInputDeviceId\n      ? AudioConfig.fromMicrophoneInput(audioInputDeviceId)\n      : AudioConfig.fromDefaultMicrophoneInput();\n  }\n\n  return ({ referenceGrammarID } = {}) => {\n    const { SpeechGrammarList, SpeechRecognition, speechSynthesis, SpeechSynthesisUtterance } = createPonyfill({\n      audioConfig,\n      audioContext,\n      credentials,\n      enableTelemetry,\n      referenceGrammars: referenceGrammarID ? [`luis/${referenceGrammarID}-PRODUCTION`] : [],\n      speechRecognitionEndpointId,\n      speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat,\n      textNormalization\n    });\n\n    return {\n      SpeechGrammarList,\n      SpeechRecognition,\n      speechSynthesis,\n      SpeechSynthesisUtterance\n    };\n  };\n}\n"],"sourceRoot":"bundle:///"},"metadata":{},"sourceType":"script"}